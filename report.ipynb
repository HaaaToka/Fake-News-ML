{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HACETTEPE UNIVERSITY COMPUTER ENGINEERING\n",
    "\n",
    "## Assignment 2\n",
    "Name & Surname : Okan ALAN\n",
    "\n",
    "School Number : 21526638\n",
    "\n",
    "Course : Introduction to Machine Learning Laboratory\n",
    "\n",
    "Advisors: Dr. Aykut Erdem, T.A. Necva Bölücü\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 THEORY QUESTIONS\n",
    "\n",
    "## MLE\n",
    "\n",
    "-- Suppose you have N samples x1,2,...xN from a univariate normal distribution with unknown mean μ and known variance σ^2. Derive the MLE estimator for the mean μ.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(x_{1}...x_{N}|\\mu) = \\prod_{i=1}^NP(x_{i}|\\mu)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\text{maximize the log-likelihood}\n",
    "P(x_{1}...x_{N}|\\mu) = \\sum_{i=1}^Nlog(\\frac{1}{\\sqrt{2\\pi\\sigma^2}})-\\frac{x_{i}-\\mu}{\\sigma^2}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\text{We take derivatives of this with respect to µ}\n",
    "\\frac{\\mathrm{d}log(P(x_{1}...x_{N}|\\mu))}{\\mathrm{d}\\mu} = \\sum_{i=1}^N\\frac{x_{i}-\\mu}{\\sigma^2}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\text{Setting the left hand side equal to zero}\n",
    "0 = \\frac{\\sum_{i=1}^Nx_{i}}{N}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\hat{\\mu} = \\frac{\\sum_{i=1}^Nx_{i}}{N}\n",
    "\\end{equation*}\n",
    "    \n",
    "-- Consider a dataset (x^n, c^n); n = 1....N of binary attributes, xi^n ∈ 0,1; i = 1....D and associated class label c^n. The number of datapoints from class c = 0 is denoted n0 and the number from class c = 1 is denoted n1. Estimate p(xi = 1jc) θi^c.\n",
    "\n",
    "-- Suppose that X is a discrete rrandom variable with the following probability mass function: where 0<=θ<=1 is a parameter. The following 10 independent observations were taken from such a distribution: (3,0,2,1,3,2,1,0,2,1). What is the maximum likelihood estimate of θ.\n",
    "\n",
    "|X|0|1|2|3|\n",
    "|----|----|---|--------|-------|\n",
    "|P(X)|2θ/3|θ/3|2(1-θ)/3|(1-θ)/3|\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ L(\\theta) = P(X=3)P(X=0)P(X=2)P(X=1)P(X=3)P(X=2)P(X=1)P(X=0)P(X=2)P(X=1)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "L(\\theta) = \\prod_{i=0}^{n}P(X_{i}|\\theta) = (\\frac{2\\theta}{3})^2 (\\frac{\\theta}{3})^3 (\\frac{2(1-\\theta)}{3})^3 (\\frac{1-theta}{3})^2\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\ L(\\theta) = 2(log(2/3)+log\\theta)+3(log(1/3)+log\\theta)+3(log((1-\\theta)/3)+log(1-\\theta))+2(log((1-\\theta)/3)+log(1-\\theta)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    " 0 = \\frac{5}{\\theta} - \\frac{5}{1-\\theta} \\iff \\theta = 0.5\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "A psychologist does a small survey on 'happiness'. Each respondent provides a vector with entries 1 or 0 corresponding to whether they answer 'yes' to a question or 'no', respectively. The question vector has attributes x = (rich, married, healthy)\n",
    "Thus, a response (1, 0, 1) would indicate that the respondent was 'rich', 'unmarried', 'healthy'. In addition, each respondent gives a value c = 1 if they are content with their lifestyle, and c = 0 if they are not. The following responses were obtained from people who claimed also to be 'content': (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1) and for 'not content': (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0).\n",
    "\n",
    "|R|M|H|y|\n",
    "|-|-|-|-|\n",
    "|1|1|1|1|\n",
    "|0|0|1|1|\n",
    "|1|1|0|1|\n",
    "|1|0|1|1|\n",
    "|0|0|0|0|\n",
    "|1|0|0|0|\n",
    "|0|0|1|0|\n",
    "|0|1|0|0|\n",
    "|0|0|0|0|\n",
    "\n",
    "<br>P(content) = 4/9 P(notContent) = 5/9\n",
    "\n",
    "- Using Naive Bayes, what is the probability that a person who is 'not rich', 'married' and 'healthy' is 'content'? \n",
    "\n",
    "Question want us calcualte P(content|(0,1,1)).<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(content|(0,1,1)) = (P((0,1,1)|content) * P(content)) / P(0,1,1)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(content|(0,1,1)) = (P((0,1,1)|content) * P(content)) / (P(notRich) * P(married) * P(healthy))\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P((0,1,1)|content) = P(notRich|content) * P(married|content) * P(healhy|content) = 1/4 * 1/2 * 3/4 = 3/32 \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(content|(0,1,1)) = (3/32 * 4/9) / (5/9 * 1/3 * 4/9) = 81/160 \\\\\n",
    "\\end{equation*}\n",
    "- What is the probability that a person who is 'not rich' and 'married' is 'content'? (That is, we do not know whether ot not they are 'healthy'.)\n",
    "\n",
    "Question want us calcualte P(content|(0,1,?)).<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(content|(0,1,?)) = P((0,1,?)|content) * P(content)/ (P((notRich)* P(married)) \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P((0,1,?)|content) = P(notRich|content) * P(married|content) = 1/4 * 1/2 = 1/8 \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(content|(0,1,?)) = 1/8 * 4/9 / ( 5/9 * 1/3) = 3/10 \\\\\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 Detection of Fake News\n",
    "\n",
    "In the Part 2, this assignment wants us to determine whether a headline is a real or fake news. While we were determining the class of news, the assignment wants us to implement a Naive Bayes classifier that is simple classification algorithm. That makes an assumption about the conditional independence of features, but it works quite well in practice.\n",
    "\n",
    "## DATASET\n",
    "\n",
    "1298 fake news headlines (which mostly include headlines of articles classified as biased etc.) and 1968 real news headlines\n",
    "\n",
    "\\# new's headlines Train-> fake=1104 real=1673<br>\n",
    "\\# new's headlines Test-> 489\n",
    "\n",
    "## APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I Understanding the data\n",
    "\n",
    "-- You will be predicting whether a headline is real or fake news from words that appear in the headline. Is that feasible? Give 3 examples of specific keywords that may be useful, together with statistics on how often they appear in real and fake headlines.\n",
    "\n",
    "I think that is feasible. There are few words to predict whether a headline is real or not.\n",
    "    1) 'ban': 1 time in fake, 65 times in real\n",
    "    2) 'breaking': 24 times in fake, 1 time in real\n",
    "    3) 'won': 16 times in fake, 1 time in real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import text\n",
    "from operator import itemgetter\n",
    "from math import log,log10,exp\n",
    "\n",
    "myStopWords=text.ENGLISH_STOP_WORDS\n",
    "ngram=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the libraries that will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeror():\n",
    "    return 0\n",
    "def zeror2():\n",
    "    return [0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create defaultdict, it needs a function as argument  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDict(corpus,ngram):\n",
    "\n",
    "    counter = defaultdict(zeror)\n",
    "\n",
    "    for doc in corpus:\n",
    "        docs = doc.split()\n",
    "        sd = []\n",
    "        for i in range(len(docs) - ngram + 1):\n",
    "            sd.append(' '.join(docs[i:i + ngram]))\n",
    "        sd=set(sd) # [1]\n",
    "        for k in sd:\n",
    "            counter[k]+=1\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate frequency of words in given text.\n",
    "[1]. when we determined the class of news, the important thing is a number of headlines with word, not counts of word. For example, the first option is one headline contain 10- times \"okan\",the second option is 10 headlines contain 1-time \"okan\". We have same counts \"okan\" that is 10 but the important thing is how many diffirent headlines containing \"okan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fake=[]\n",
    "corpus_real=[]\n",
    "count_realNews=0\n",
    "count_fakeNews=0\n",
    "\n",
    "with open('clean_fake-Train.txt') as FileObj:\n",
    "    for line in FileObj:\n",
    "        count_fakeNews+=1\n",
    "        corpus_fake.append(line.strip())\n",
    "\n",
    "with open('clean_real-Train.txt') as FileObj:\n",
    "    for line in FileObj:\n",
    "        count_realNews+=1\n",
    "        corpus_real.append(line.strip())\n",
    "\n",
    "fakeVocab=createDict(corpus_fake,ngram)\n",
    "\n",
    "realVocab=createDict(corpus_real,ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading file and creating vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionVocab(fakeV,realV,words):\n",
    "    \n",
    "    uni = dict()\n",
    "\n",
    "    for word in words:\n",
    "        if word not in fakeV:\n",
    "            uni[word]=(0,realV[word])\n",
    "        elif word not in realV:\n",
    "            uni[word]=(fakeV[word],0)\n",
    "        else:\n",
    "            uni[word]=(fakeV[word],realV[word])\n",
    "\n",
    "    return uni\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function merge real and fake vocabulary. I store fake and real words counts together because if I keep it separately I'll spend extra memory and there are already have too many variables,I don't want to double it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabTrain=unionVocab(fakeVocab,realVocab,set(fakeVocab.keys())|set(realVocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test=[]\n",
    "count_testNews=0\n",
    "\n",
    "with open('test.csv') as FileObj:\n",
    "    FileObj.readline()\n",
    "    for line in FileObj:\n",
    "        count_testNews+=1\n",
    "        corpus_test.append(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading test.csv to determine whether the news is real or fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II Implementing Naive Bayes\n",
    "\n",
    "I have to implement our own Naive Bayes algorthm in this part. I use Bag of Words (BoW) model which learns a vocabulary\n",
    "from all of the documents, then models each document by counting the number of times each word appears.  use BoW with two options:\n",
    "- Unigram: The occurrences of words in a document(frequency of the word).\n",
    "- Bigram: The occurrences of two adjacent words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeCount=sum([x for x in fakeVocab.values()])\n",
    "realCount=sum([x for x in realVocab.values()])\n",
    "vocCount=len(vocabTrain.items())\n",
    "P_real = count_realNews/(count_realNews+count_fakeNews)\n",
    "P_fake = 1- P_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I count words in documents. Then I calculate probablity of real and fake news. -P(fake) and P(real)-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalProbabilty(cr,cf,vocab,wordsCount):\n",
    "\n",
    "    cp={}\n",
    "\n",
    "    for key,val in vocab.items():\n",
    "        cp[key] = ( (val[0]+1)/(cf+wordsCount) , (val[1]+1)/(cr+wordsCount) )\n",
    "\n",
    "    return cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace=conditionalProbabilty(realCount,fakeCount,vocabTrain,vocCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need probability of a word  given its class(P(word|fake) or P(word|real)) to use into the Naive Bayes. I use add-one-smoothing in this part. Add-one-smoothing :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{P}(word|class) = \\frac{ count(word,class)+1 }{ count(word)+ |V|}\n",
    "\\end{equation*}\n",
    "\n",
    "Laplace is { word: ( P_hat(word|fake), P_hat(word|real) ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(testNews,lap,pr,pf,cr,cf,cw):\n",
    "\n",
    "    P_word_fake = 0\n",
    "    P_word_real= 0\n",
    "    for word in testNews:\n",
    "        if word in lap.keys():\n",
    "            P_word_fake += log10(lap[word][0])\n",
    "            P_word_real += log10(lap[word][1])\n",
    "        else:\n",
    "            P_word_fake += log10(1/(cf+cw))\n",
    "            P_word_real += log10(1/(cr+cw))\n",
    "\n",
    "    P_fake_words = pf + P_word_fake\n",
    "    P_real_words = pr + P_word_real\n",
    "\n",
    "    return 'fake' if P_fake_words>P_real_words else 'real'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my Naive Bayes function. Bayes Rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\theta|D) = \\frac{P(D|\\theta)*P(\\theta)}{P(D)}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(\\theta|D) \\propto P(D|\\theta)*P(\\theta)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(\\theta) = P(D|\\theta)*P(\\theta) + P(D|\\theta')*P(\\theta')\n",
    "\\end{equation*}\n",
    "\n",
    "<br>P(θ|D) is posterior , P(D|θ) is likelihood, P(θ) is prior. In my assignment θ assign real and fake classes. \n",
    "\n",
    "\\begin{equation*}\n",
    "P(fake|word_{1},word_{2}...word_{n}) \\propto P(fake) * \\prod_{i=1}^{n}P(word_{i}|fake) \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(fake|word_{1},word_{2}...word_{n}) = \\frac{P(fake) * \\prod_{i=1}^{n}P(word_{i}|fake)}{P(fake) * \\prod_{i=1}^{n}P(word_{i}|fake) + P(real) * \\prod_{i=1}^{n}P(word_{i}|real)}\n",
    "\\end{equation*}\n",
    "\n",
    "<br> I compute the log probabilities to prevent numerical underflow when calculating multiplicative probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III\n",
    "\n",
    "#### a) Analyzing Effect Of The Words On Prediction\n",
    "\n",
    "- List the 10 words whose presence most strongly predicts that the news is real.\n",
    "- List the 10 words whose absence most strongly predicts that the news is real.\n",
    "- List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "- List the 10 words whose absence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,pro in vocabTrain.items():\n",
    "    if pro[0]==0:\n",
    "        vocabTrain[word]=(1,vocabTrain[word][1])\n",
    "    elif pro[1]==0:\n",
    "        vocabTrain[word]=(vocabTrain[word][0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words appear in only real headlines, some words appear in only fake headlines. This is a problem because when we calculate  P(word|class), it results is 0. This is caused problem while finding absence and presence words. Therefore I add 1 to every 0 value. It looks like  add-one-smoothing but adding 1 only to 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilty(word_count_dict,countReal,countFake):\n",
    "    \n",
    "    dc=defaultdict(zeror2)\n",
    "\n",
    "    for word,counts in word_count_dict.items():\n",
    "        dc[word][0]=counts[0]/countFake\n",
    "        dc[word][1]=counts[1]/countReal\n",
    "\n",
    "    return dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating P(word|class) for absences and presences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PresenceAbsence(pr,pf,fnewa,rnews,vocab):\n",
    "    P_fake_given_word={}\n",
    "    P_fake_given_not_word={}\n",
    "    P_real_given_word={}\n",
    "    P_real_given_not_word={}\n",
    "\n",
    "    prob=probabilty(vocab,rnews,fnewa)\n",
    "    p_word={}\n",
    "    not_word = {}\n",
    "    not_prob={}\n",
    "\n",
    "    global count_realNews,count_fakeNews\n",
    "    tp=count_realNews+count_fakeNews\n",
    "    for key,pro in prob.items():\n",
    "        _over = (pro[0]*pf)+(pro[1]*pr)\n",
    "        P_fake_given_word[key] = (pro[0]*pf)/_over\n",
    "        P_real_given_word[key] = (pro[1]*pr)/_over\n",
    "        p_word[key] = sum(pro)/tp\n",
    "\n",
    "        not_prob[key] = [1-pro[0],1-pro[1]]\n",
    "        not_word[key]=1-p_word[key]\n",
    "\n",
    "    for key,pro in not_prob.items():\n",
    "        P_fake_given_not_word[key] = (pro[0]*pf)/not_word[key]\n",
    "        P_real_given_not_word[key] = (pro[1]*pr)/not_word[key]\n",
    "\n",
    "    return P_fake_given_word, P_fake_given_not_word, P_real_given_word, P_real_given_not_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_fake,absence_fake,presence_real,absence_real=PresenceAbsence(P_real,P_fake,count_fakeNews,count_realNews,vocabTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculate below equailities. These are help us to find absence and presence words.<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "P(class) = \\frac{\\text{# of headlines of that class}}{\\text{total # of headlines}}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(word | class) = \\frac{\\text{ # of occurrences of word in headlines of that class}}{\\text{total # headlines of that class}}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(class | word) = \\frac{P(word | class) * P(class)} { ( P(word|fake) * P(fake) + P(word|real) *P(real) }\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(class | \\neg word) = \\frac{P(\\neg word | class)P(class)}{P(\\neg word)}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "P(\\neg word | class) = 1 - P(word | class)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\neg P(word) = \\frac{\\text{headlines without word}}{\\# \\text{headlines}}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\n",
    "After applying formulas to our data set, I obtain presence and absence dictionary. Presence dictionary is contain class given word {word: P(class|word)}. Presence dictionary is contain class given not word {word: P(class|-word)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_real1 = sorted(list(presence_real.items()), key=itemgetter(1))[::-1]\n",
    "absence_real1 = sorted(list(absence_real.items()), key=itemgetter(1))[::-1]\n",
    "presence_fake1 = sorted(list(presence_fake.items()), key=itemgetter(1))[::-1]\n",
    "absence_fake1 = sorted(list(absence_fake.items()), key=itemgetter(1))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries doesn't store sorted key,value pair. That's why I sort them. We can see the first 10 elements from sorted list.\n",
    "\n",
    "\n",
    "|Presence Real|Absence Real|Presence Fake|Absence Fake|\n",
    "|---|---|---|---|\n",
    "|ban|black|black|ban|\n",
    "|korea|breaking|breaking|korea|\n",
    "|turnbull|soros|soros|turnbull|\n",
    "|travel|won|won|travel|\n",
    "|trumps|3|watch|australia|\n",
    "|north|u|3|comments|\n",
    "|australia|political|u|climate|\n",
    "|comments|m|political|tax|\n",
    "|climate|daily|m|paris|\n",
    "|tax|reporter|daily|refugee|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Stopwords\n",
    "\n",
    "myStopWords={'would', 'back', 'do', 'hereupon', 'ie', 'often', 'only', 'how', 'there', 'cannot', 'done', 'whether', 'anyway', 'became', 'fifty', 'hereafter', 'few', 'made', 'not', 'but', 'very', 'we', 'enough', 'detail', 'she', 'yourself', 'amongst', 'has', 'up', 'beforehand', 'noone', 'every', 'which', 'over', 'above', 'somewhere', 'meanwhile', 'either', 'now', 'well', 'describe', 'go', 'then', 'next', 'my', 'sixty', 'twelve', 'mill', 'top', 'becomes', 'see', 'he', 'seemed', 'be', 'name', 'each', 'its', 'whereas', 'whom', 'itself', 'might', 'about', 'off', 'below', 'everywhere', 'forty', 'being', 'must', 'herein', 're', 'sometimes', 'put', 'third', 'too', 'is', 'thereby', 'get', 'ever', 'may', 'under', 'us', 'as', 'why', 'anyone', 'ltd', 'hereby', 'than', 'thereafter', 'were', 'bottom', 'con', 'here', 'un', 'hundred', 'latter', 'should', 'that', 'formerly', 'one', 'such', 'mine', 'around', 'eight', 'me', 'eg', 'no', 'are', 'i', 'it', 'him', 'much', 'the', 'his', 'what', 'through', 'from', 'further', 'within', 'neither', 'sincere', 'throughout', 'amoungst', 'fifteen', 'or', 'moreover', 'upon', 'you', 'been', 'already', 'alone', 'same', 'hasnt', 'thru', 'via', 'whereby', 'himself', 'someone', 'none', 'nowhere', 'due', 'always', 'these', 'whoever', 'therefore', 'because', 'beside', 'behind', 'side', 'some', 'somehow', 'along', 'this', 'wherever', 'onto', 'themselves', 'de', 'an', 'among', 'four', 'any', 'give', 'together', 'bill', 'also', 'afterwards', 'both', 'never', 'our', 'against', 'couldnt', 'although', 'least', 'towards', 'herself', 'since', 'except', 'all', 'become', 'at', 'almost', 'call', 'found', 'thick', 'whereupon', 'whole', 'before', 'otherwise', 'to', 'across', 'empty', 'if', 'whatever', 'thereupon', 'out', 'another', 'during', 'their', 'yours', 'everything', 'cry', 'amount', 'move', 'less', 'fire', 'indeed', 'into', 'in', 'show', 'take', 'will', 'with', 'your', 'ten', 'hers', 'and', 'anything', 'ours', 'per', 'thin', 'was', 'nor', 'ourselves', 'can', 'six', 'between', 'who', 'for', 'by', 'elsewhere', 'find', 'however', 'myself', 'on', 'something', 'other', 'hence', 'eleven', 'interest', 'please', 'without', 'own', 'yet', 'many', 'therein', 'perhaps', 'those', 'two', 'co', 'anyhow', 'toward', 'first', 'everyone', 'five', 'beyond', 'am', 'cant', 'even', 'front', 'rather', 'seem', 'them', 'though', 'inc', 'a', 'where', 'while', 'thus', 'anywhere', 'have', 'fill', 'still', 'whenever', 'etc', 'after', 'wherein', 'nothing', 'keep', 'once', 'nobody', 'could', 'besides', 'part', 'thence', 'last', 'whither', 'whose', 'more', 'until', 'her', 'yourselves', 'most', 'seems', 'had', 'namely', 'system', 'three', 'becoming', 'when', 'full', 'nevertheless', 'so', 'others', 'former', 'several', 'they', 'whence', 'sometime', 'latterly', 'nine', 'serious', 'whereafter', 'twenty', 'seeming', 'mostly', 'else', 'again', 'of', 'down'}\n",
    "\n",
    "There is no stopwords in the top ten words lists Same lists at part3(a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Analyzing efect of the stopwords\n",
    "\n",
    "Stop words don't affect my results Part3(b). For example if I increase ngram, what will happen. Lets see.\n",
    "\n",
    "|Presence Fake|without stopwords Presence Fake|\n",
    "|---|---|\n",
    "|if trump|trump supporter|\n",
    "|trump supporter|daily wire|\n",
    "|comment on|trump won|\n",
    "|i m|george soros|\n",
    "|voting for|world war|\n",
    "|daily wire|endingfed news|\n",
    "|trump won|news network|\n",
    "|of fame|breaking trump|\n",
    "|will win|fame star|\n",
    "|why trump|pro trump|\n",
    "\n",
    "<br>Without 3 pair, all of them changed. Removing or keeping stop words might make sense our model. Stop words gives us relationship between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IIII Calculation of Accuracy\n",
    "\n",
    "\\begin{equation*}\n",
    "accuracy = \\frac{ \\text{# correctly classified examples} }{ \\text{# examples} }\n",
    "\\end{equation*}\n",
    "\n",
    "accuracy is 86.70756646216769 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.70756646216769\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for document in corpus_test:\n",
    "    doc=createDict([document[0]],ngram).keys()\n",
    "    pre=naiveBayes(doc,laplace,P_real,P_fake,realCount,fakeCount,vocCount)\n",
    "    if document[1]==pre:\n",
    "        acc+=1\n",
    "print(acc*100/count_testNews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
